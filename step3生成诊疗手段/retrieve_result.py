import json
import os
import re

# ================= é…ç½®åŒºåŸŸ =================
BATCH_RESULT_FILE = "batch_output_kimi_step3.jsonl"
INPUT_DIR = "../è¯Šç–—æŒ‡å—æ•´åˆstep2_kimiï¼ˆåˆ›å»ºç—…ä¾‹ï¼‰"
OUTPUT_DIR = "../è¯Šç–—æŒ‡å—æ•´åˆstep3_kimiï¼ˆç”Ÿæˆè¯Šç–—æ‰‹æ®µï¼‰"
ERROR_LOG_FILE = "error_records_kimi_step3.txt"
# ===========================================

def clean_llm_json(text):
    """
    æ¸…æ´— LLM è¿”å›çš„æ–‡æœ¬ï¼Œæå–åˆæ³•çš„ JSON éƒ¨åˆ†ã€‚
    é’ˆå¯¹ DeepSeek R1 ç­‰æ¨¡å‹ï¼Œå¯èƒ½ä¼šåŒ…å«æ€ç»´é“¾å†…å®¹ï¼Œéœ€ç²¾å‡†æå– JSONã€‚
    """
    if not text: return None
    try:
        # 1. å°è¯•ç›´æ¥è§£æ
        return json.loads(text)
    except json.JSONDecodeError:
        # 2. å°è¯•æå– Markdown ä»£ç å— ```json ... ```
        match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
        if match:
            try: return json.loads(match.group(1))
            except: pass
        
        # 3. å°è¯•æå–çº¯å¤§æ‹¬å·å†…å®¹ { ... }
        # è´ªå©ªåŒ¹é…ï¼šä»ç¬¬ä¸€ä¸ª { åˆ° æœ€åä¸€ä¸ª }
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            try: return json.loads(match.group(0))
            except: pass
            
        return None

def log_error(f_err, error_type, custom_id, message, raw_data=None):
    """
    å°†é”™è¯¯å†™å…¥æ—¥å¿—æ–‡ä»¶
    """
    f_err.write(f"[{error_type}] ID: {custom_id}\n")
    f_err.write(f"Message: {message}\n")
    if raw_data:
        f_err.write(f"Raw Data: {raw_data[:200]}...\n") 
    f_err.write("-" * 50 + "\n")

def process_merge_results():
    # 0. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(BATCH_RESULT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ‰¹é‡ç»“æœæ–‡ä»¶ {BATCH_RESULT_FILE}")
        return

    # 1. [é€’å½’é¢„åŠ è½½] Step 2 çš„æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
    # ç›®çš„ï¼šé€šè¿‡ custom_id ä¸­çš„è·¯å¾„å’Œç´¢å¼•ï¼Œæ‰¾åˆ°åŸå§‹çš„ case_input å’Œ reference_rule
    print(f"ğŸ“‚ æ­£åœ¨é€’å½’æ‰«æ {INPUT_DIR} åŠ è½½ Step 2 æºæ•°æ®...")
    step2_data_cache = {}
    
    if os.path.exists(INPUT_DIR):
        for root, dirs, files in os.walk(INPUT_DIR):
            for file in files:
                if file.endswith(".json"):
                    full_path = os.path.join(root, file)
                    # è·å–ç›¸å¯¹è·¯å¾„ï¼Œä¾‹å¦‚ï¼š "å„¿ç§‘/å„¿ç«¥çŒ´ç—˜è¯Šç–—å’Œé¢„é˜²ä¸“å®¶å…±è¯†.json"
                    rel_path = os.path.relpath(full_path, INPUT_DIR).replace("\\", "/")
                    
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            # ç¡®ä¿ç¼“å­˜çš„æ˜¯åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶åªæœ‰å•ä¸ªå¯¹è±¡ï¼Œè½¬ä¸ºåˆ—è¡¨
                            if isinstance(data, list):
                                step2_data_cache[rel_path] = data
                            else:
                                step2_data_cache[rel_path] = [data]
                    except Exception as e:
                        print(f"[Warn] æ— æ³•è¯»å–æºæ–‡ä»¶ {rel_path}: {e}")
    else:
        print(f"âŒ é”™è¯¯: æºæ•°æ®ç›®å½• {INPUT_DIR} ä¸å­˜åœ¨ï¼")
        return

    print(f"âœ… å·²ç¼“å­˜ {len(step2_data_cache)} ä¸ª Step 2 æ–‡ä»¶ã€‚")

    # 2. [æµå¼å¤„ç†] è¯»å– Batch ç»“æœå¹¶èåˆ
    print(f"ğŸš€ æ­£åœ¨å¤„ç† {BATCH_RESULT_FILE} ...")
    final_results = {} 
    success_count = 0
    fail_count = 0

    with open(BATCH_RESULT_FILE, 'r', encoding='utf-8') as f_in, \
         open(ERROR_LOG_FILE, 'w', encoding='utf-8') as f_err:
        
        for line_num, line in enumerate(f_in):
            if not line.strip(): continue
            
            custom_id = f"Line_{line_num}"
            
            try:
                res_item = json.loads(line)
                custom_id = res_item.get('custom_id', f"Unknown_ID_Line_{line_num}")
                
                # A. è§£æ ID (æ ¼å¼ï¼šç›¸å¯¹è·¯å¾„|ç´¢å¼•)
                if '|' in custom_id:
                    relative_path, original_idx_str = custom_id.rsplit('|', 1)
                    original_idx = int(original_idx_str)
                else:
                    log_error(f_err, "ID Format Error", custom_id, "ç¼ºå°‘ '|' åˆ†éš”ç¬¦", line)
                    fail_count += 1
                    continue

                # B. æ£€æŸ¥ API é”™è¯¯
                if res_item.get('error'):
                    err_msg = str(res_item['error'])
                    print(f"[API Error] {custom_id}: {err_msg}")
                    log_error(f_err, "API Error", custom_id, err_msg)
                    fail_count += 1
                    continue
                
                # C. è§£æ DeepSeek è¿”å›çš„å†…å®¹
                llm_json = None
                generated_content = ""
                try:
                    # è·å– content å­—æ®µ (DeepSeek R1 çš„ output é€šå¸¸åœ¨ choices[0].message.content)
                    generated_content = res_item['response']['body']['choices'][0]['message']['content']
                    llm_json = clean_llm_json(generated_content)
                except Exception as e:
                    log_error(f_err, "Content Parse Error", custom_id, str(e), str(res_item)[:200])
                    fail_count += 1
                    continue

                if not llm_json: 
                    log_error(f_err, "JSON Extraction Failed", custom_id, "æ— æ³•æå–æœ‰æ•ˆ JSON", generated_content)
                    fail_count += 1
                    continue

                # D. ä»ç¼“å­˜ä¸­è·å– Step 2 çš„åŸå§‹æ•°æ®
                source_record = {}
                if relative_path in step2_data_cache:
                    data_list = step2_data_cache[relative_path]
                    if 0 <= original_idx < len(data_list):
                        source_record = data_list[original_idx]
                    else:
                        msg = f"ç´¢å¼•è¶Šç•Œ: Index {original_idx} >= Length {len(data_list)}"
                        log_error(f_err, "Index Out of Bounds", custom_id, msg)
                        fail_count += 1
                        continue
                else:
                    msg = f"æ‰¾ä¸åˆ°æºæ–‡ä»¶ç¼“å­˜: {relative_path}"
                    log_error(f_err, "Source File Missing", custom_id, msg)
                    fail_count += 1
                    continue

                # E. ç»„è£…æœ€ç»ˆæ•°æ® (Step 3 æ ¼å¼)
                # ä¿ç•™ Step 2 çš„ id, file_name, case_input, reference_rule
                # æ–°å¢ thought, medical_order, patient_dialogue
                final_record = {
                    "id": source_record.get("id"), # ä¿æŒ ID ä¸€è‡´æ€§
                    "file_name": source_record.get("file_name"),
                    "case_input": source_record.get("case_input"),
                    "reference_rule": source_record.get("reference_rule"),
                    # --- DeepSeek ç”Ÿæˆçš„æ–°å­—æ®µ ---
                    "thought": llm_json.get("thought"),
                    "medical_order": llm_json.get("medical_order"),
                    "patient_dialogue": llm_json.get("patient_dialogue")
                }

                # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„å­˜å‚¨
                if relative_path not in final_results:
                    final_results[relative_path] = []
                
                final_results[relative_path].append(final_record)
                success_count += 1

            except json.JSONDecodeError:
                log_error(f_err, "JSONL Line Error", custom_id, "Line not valid JSON", line)
                fail_count += 1
            except Exception as e:
                print(f"[Unknown Error] {custom_id}: {e}")
                log_error(f_err, "Unknown Error", custom_id, str(e))
                fail_count += 1

    # 3. [åˆ†å‘ä¿å­˜] ä¿æŒç›®å½•ç»“æ„å†™å…¥æ–‡ä»¶
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡ä»¶åˆ° {OUTPUT_DIR} ...")
    
    for relative_path, records in final_results.items():
        output_file_path = os.path.join(OUTPUT_DIR, relative_path)
        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
        
        # ä¸ºäº†ç¾è§‚ï¼ŒæŒ‰ ID æ’åºï¼ˆå¯é€‰ï¼‰
        # records.sort(key=lambda x: x.get('id', 0))

        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        print(f"  â””â”€ å·²ä¿å­˜: {output_file_path} ({len(records)} æ¡)")

    print(f"\n{'='*30}")
    print(f"å¤„ç†å®Œæˆï¼")
    print(f"âœ… æˆåŠŸåˆå¹¶: {success_count} æ¡")
    print(f"âŒ å¤±è´¥/è·³è¿‡: {fail_count} æ¡ (è¯¦æƒ…è§ {ERROR_LOG_FILE})")
    print(f"ğŸ“‚ ç»“æœå·²å­˜å…¥: {OUTPUT_DIR}")
    print(f"{'='*30}")

if __name__ == "__main__":
    process_merge_results()